>vector encoding
for i ← 0…(last.length) do
  last[i] ← encoders[last[i]]
end for

>add identity vector
for i ← 0…(last.length) do
   last[i] ← last[i] + identities[i]
end for

>positional encoding
for i ← 0…(last.length) do
   last[i] ← last[i] + TENSOR(POSITIONERS(i),SHAPE(last[i]))
end for

>pgap model
for i ← 0…(layers) do
  kprod ← TENSOR(0,SHAPE(last))
  >run attention networks
  for j ← 0…(last.length) do
    >output is [final layer,full unactivated network]
    output ← LINEAR(last[j],weights[i][j],biases[i][j])
    >scale output[0] to avg 0
    output[0] ← output[0] - TENSOR(0.5,SHAPE(output[0]))

    >update vector encoding
    last[j] ← output[0].SLICE(0,esize)

    >assign attention value
    kprod[j] ← output[0].SLICE(esize,2*esize)

    >store unactivated for backpropagation
    neuronstore[i][j] ← output[1]
  end for

  >run attention network
  atten ← LINEAR(CONCAT(kprod),apweights[ll],apbiases[ll])
  >scale entire last layer to avg 0
  atten[0] ← atten[0] - TENSOR(0.5,SHAPE(atten[0]))

  >store unactivated for backpropagation
  apneur[i] ← atten[1]

  for j ← 0…(last.length) do
    >assign slice of attention pattern
    patt ← atten[0].SLICE(j*esize,(j+1)*esize)

    >add corresponding section of attention to last
    last[j] ← last[j] + patt
  end for
end for

>decode all iteration outputs
for i ← 0…(last.length) do
  >run decoding network without bias
  encout ← LINEAR(last[i],TRANSPOSE(encoders))
  >norm and softmax output
  last[i] = SOFTMAX(NORMALIZE(encout[0],1),smtemperature)
  >store for backpropagation
  encneur[i] = encout[1]  
end for

return last
